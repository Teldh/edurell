import requests, re, ast
from nltk import WordNetLemmatizer
from conllu import parse

from db_mongo import insert_conll_MongoDB, get_conll
import string

def conll_gen(video_id,text):
    """
    Takes a text as input and return a conll file
    :param text: txt
    :return sentence: conll of the text
    """
    conll = get_conll(video_id)
    if conll is None:

        files = {
            'data': text,
            'model': (None, 'english-ewt-ud-2.4-190531'),
            'tokenizer': (None, ''),
            'tagger': (None, ''),
            'parser': (None, ''),
        }

        r = requests.post('http://lindat.mff.cuni.cz/services/udpipe/api/process', files=files)
        re = r.json()

        #print(re['result'])

        # json da salvare
        data = {
            'video_id':video_id,
            'conll':re['result']
        }
        #print(re["result"])
        insert_conll_MongoDB(data)
        return parse(re['result'])
    else:
        return parse(conll)


# get text from a conll in the db
def get_text(video_id, return_conll=False):
    conll = get_conll(video_id)
    if conll is not None:
        parsed = parse(conll)
        text = ""
        for sentence in parsed:
            text += sentence.metadata["text"] + " "
            if return_conll:
                return text, conll
        return text
    return None


# get text from a conll in the db
def get_text_for_keywords(video_id):
    conll = get_conll(video_id)
    if conll is not None:
        parsed = parse(conll)
        text = ""
        for sentence in parsed:
            for word in sentence:
                if word["upos"] in ['NOUN', 'PROPN', 'ADJ']:
                    text += word["lemma"] + " "

        return text
    return None

def lemmatize(lemmas):
    concepts_lemmatized = []

    lemmatizer = WordNetLemmatizer()

    for concept in lemmas:
        lemmatized = ""

        for word in concept.split(" "):
            lemmatized += lemmatizer.lemmatize(word.lower()) + " "

        lemmatized = lemmatized.rstrip()

        if lemmatized not in concepts_lemmatized:
            concepts_lemmatized.append(lemmatized)

    return concepts_lemmatized


# def create_text(subtitles, autogenerated, conll_sentences):
#     lemmatizer = WordNetLemmatizer()
#
#     sent_id = 0
#     word_id = 0
#     word_counter = 0
#
#     all_lemmas = []
#     lemmatized_subtitles = []
#
#     for sub in subtitles:
#         sent = {"text": ""}
#         text = sub["text"]
#         text = text.replace("\n", " ").replace("…", " … ")
#
#         # aggiungo uno spazio vuoto prima e dopo la punteggiatura
#         text = re.sub('([.,!?():])', r' \1 ', text)
#         text = re.sub('\s{2,}', ' ', text)
#         text = text.replace("/", " / ").replace("-", " - ")
#         text = text.replace("'", " '").replace("’", " ’")
#
#         text_words = text.split(" ")
#
#         for w in text_words:
#             sentence_finished = False
#             if w != '':
#                 text_word = w.lower()
#
#                 if not autogenerated:
#                     conll_words = conll_sentences[sent_id]
#                 else:
#                     conll_words = conll_sentences[sent_id].filter(upos=lambda x: x != "PUNCT")
#
#                 for i in range(word_counter, len(conll_words)):
#
#                     conll_word = str(conll_words[i]["form"]).lower()
#                     #print(conll_word, text_word)
#
#                     if text_word == conll_word:
#
#                         word_id = conll_words[i]["id"]
#
#                         if not autogenerated:
#                             word_counter = word_id
#                         else:
#                             word_counter += 1
#
#                         if conll_words[i]["id"] == conll_words[-1]["id"]:
#                             sentence_finished = True
#                         break
#
#                 lemma = lemmatizer.lemmatize(w.lower())
#                 sent["text"] += '<span lemma="' + lemma + '" sent_id="' + str(sent_id + 1) + '" word_id="' + str(
#                     word_id) + '" >' + w + '</span> '
#
#                 if lemma not in all_lemmas:
#                     all_lemmas.append(lemma)
#
#                 if sentence_finished:
#                     sent_id += 1
#                     word_id = 0
#                     word_counter = 0
#
#         lemmatized_subtitles.append(sent)
#     print("Done")
#     print(sent_id)
#
#     return lemmatized_subtitles, all_lemmas

def create_text(subtitles, autogenerated, conll_sentences):
    """
    Creation of the transcript, every word is a span element with attributes sent_id and word_id, corresponding to the
    sentence id and word id of the conll.
    """
    lemmatizer = WordNetLemmatizer()

    sent_id = 0
    word_id = 0
    word_counter = 0

    all_lemmas = []
    lemmatized_subtitles = []

    for sub in subtitles:
        sent = {"text": ""}
        text = sub["text"]
        text = text.replace("\n", " ").replace("’", " ’").replace("'", " '")



        # # aggiungo uno spazio vuoto prima e dopo la punteggiatura
        # text = re.sub('([.,!?():])', r' \1 ', text)
        # text = re.sub('\s{2,}', ' ', text)
        text = text.replace("/", " / ")#.replace("-", " - ")
        text = text.replace("'", " '").replace("’", " ’").replace("”", " ” ").replace("“", " “ ")

        #text_words = text.split(" ")
        text_words = re.split(' |-', text)
        #print(text_words)
        for w in text_words:
            sentence_finished = False
            if w != '':

                if w not in [".",":","?","!",",",";","/","“","'",'"',"”"]:

                    text_word = w.lower().translate(str.maketrans('', '', string.punctuation))\
                        .replace('”', '').replace("“", "").replace('…', '')

                    conll_words = conll_sentences[sent_id].filter(upos=lambda x: x != "PUNCT")

                    for i, c in enumerate(conll_words):
                        if "-" in c["form"]:
                            conll_words.insert(i, c.copy())
                            conll_words[i]["form"] = conll_words[i]["form"].split("-")[0]
                            conll_words[i+1]["form"] = conll_words[i+1]["form"].split("-")[1]

                    for i in range(word_counter, len(conll_words)):

                        conll_word = str(conll_words[i]["form"]).lower().translate(str.maketrans('', '', string.punctuation))
                        #print( conll_word, text_word)

                        if text_word == conll_word:
                            word_id = conll_words[i]["id"]
                            word_counter += 1

                            if conll_words[i]["id"] == conll_words[-1]["id"]:
                                sentence_finished = True
                            break

                if w in ["!", "?", "."]:
                    s = sent_id
                else:
                    s = sent_id + 1

                #print(s)
                toLemmatize = w.lower()
                if toLemmatize[-1] in ["?", ".", "!", ";", ","]:
                    toLemmatize = toLemmatize[:-1]

                lemma = lemmatizer.lemmatize(toLemmatize)

                sent["text"] += '<span lemma="' + lemma + '" sent_id="' + str(s) + '" word_id="' + str(
                    word_id) + '" >' + w + '</span> '


                if lemma not in all_lemmas:
                    all_lemmas.append(lemma)

                if sentence_finished:
                    sent_id += 1
                    word_id = 0
                    word_counter = 0

        lemmatized_subtitles.append(sent)
    print("Done")
    print(sent_id)

    return lemmatized_subtitles, all_lemmas

if __name__ == '__main__':
    print("test")